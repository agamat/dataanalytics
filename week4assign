PROBLEM NO. 1

#Read .csv file
gerber <- read.csv("gerber.csv") 

#Calculate baseline model
table(gerber$voting)
108696/344084

#Create linear model
lm1 <- lm(voting ~ hawthorne+self+neighbors+civicduty, data=gerber)

#Predict linear model and calculate prediction accuracy
predlm1 <- predict(lm1)
table(gerber$voting, predlm1)

#Create another linear model
lm2 <- lm(voting ~ hawthorne+self+neighbors+civicduty, data=gerber, family=binomial)
predlm1 <- predict(lm1, type="response")

#Check model accuracy
table(gerber$voting, predlm1>=0.3)
(134513+51966)/(134513+51966+56730+100875)
table(gerber$voting, predlm1>=0.5)
235388/(235388+108696)

#Load package and predict auc value
library(ROCR)
rocrpred1 <- prediction(predlm1, gerber$voting)
as.numeric(performance(rocrpred1, "auc")@y.values)

#Make CART model
library(rpart)
library(rpart.plot)
cartmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(cartmodel) #Plot CART model

#Make CART model with cp parameter
cartmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(cartmodel)

#Make CART model with cp parameter and another variable 'sex'
cartmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(cartmodel2)
table(gerber$control, gerber$sex)
cartmodel3 = rpart(voting ~ control, data=vote, cp=0.0)
cartmodel3 = rpart(voting ~ control, data=gerber, cp=0.0)
prp(cartmodel3, digits=6)
cartmodel4 = rpart(voting ~ control+sex, data=gerber, cp=0.0)
prp(cartmodel4, digits=6)

#Make linear model
lm4 = lm(voting ~ control+sex, data=gerber, cp=0.0)
summary(lm4)
possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(lm4, newdata=possibilities, type="response")

#Make logistic regression model
lm4 = glm(voting ~ control+sex, data=gerber, cp=0.0)
lm4 = glm(voting ~ control+sex, data=gerber)
predict(lm4, newdata=possibilities, type="response")
glm1 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(glm1)
predict(glm1, newdata=possibilities, type="response")

#PROBLEM NO. 2

#Read data and create new variable
letters <- read.csv("letters_ABPR.csv")
letters$isB = as.factor(letters$letter == "B")

#Split data into testing and training set
set.seed(1000)
library(caTools)
split <- sample.split(letters$isB, SplitRatio=0.5)
train <- subset(letters, split==TRUE)
test <- subset(letters, split==FALSE)

#Check baseline model accuracy
table(letters$isB)
2350/(2350+766)

#Create CART model
cartb = rpart(isB ~ . - letter, data=train, method="class")
pred <- predict(cartb, newdata=test, type="class")

#Check model accuracy
table(test$letters$isB, pred)
table(test$isB, pred)
(340+1118)/(340+1118+100)

#Create Random Forest model
set.seed(1000)
forestb = randomforest(isB ~ . - letter, data=train)
forestb = randomForest(isB ~ . - letter, data=train)
predforest <- predict(forestb, newdata=test)

#Check model
table(test$isB, predforest)
(374+1165)/(374+1165+19)
letters$letter = as.factor( letters$letter ) 

#Split data into training and testing set
set.seed(2000)
split <- sample.split(letters$letter, SplitRatio=0.5)
train <- subset(letters, split==TRUE)
test <- subset(letters, split==FALSE)

#Check baseline model accuracy
table(letters$letter)
401/nrow(test)

#Create CART model
cartletter = rpart(letter ~ . - isB, data=train, method="class")
predictcart <- predict(cartletter, newdata=test, type="class")
table(test$letter, predictcart)
(348+318+363+340)/nrow(test)

#Create Random Forest model
forestletter = randomForest(letter ~ . - isB, data=train, method="class")
forestletter = randomForest(letter ~ . - isB, data=train)
predictforest <- predict(forestletter, newdata=test)

#Check prediction accuracy
table(test$letter, predictforest)
(390+379+393+367)/nrow(test) 
